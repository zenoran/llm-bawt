[project]
name = "llm-bawt"
version = "0.1.0"
description = "llm-bawt - Model-agnostic LLM platform providing a unified, OpenAI-compatible API for configurable chatbots and multi-agent systems across cloud and local models."
readme = "README.md"
requires-python = ">=3.12"

dependencies = [
    "rich>=13.9.4",
    "requests>=2.25.0",
    "pydantic-settings>=2.8.1",
    "openai>=1.69.0",
    "pyyaml>=6.0.2",
    "pytz>=2025.2",
    "tiktoken>=0.9.0",
    "click>=8.0.0",
    # PostgreSQL memory backend
    "sqlalchemy>=2.0.0",
    "sqlmodel>=0.0.31",
    "psycopg2-binary>=2.9.0",
    "pgvector>=0.3.0",
    # TUI
    "textual>=0.52.0",
    "textual-dev>=1.5.0",
    "pyperclip>=1.8.0",
    "ipython>=8.0.0",
]

[project.optional-dependencies]
dev = [
    "ruff",
    "mypy",
    "pytest>=8.3.5",
    "pytest-cov>=5.0.0",
    "pytest-mock>=3.14.0",
]
memory = [
    "sentence-transformers>=2.2.0",
]
huggingface = [
    "transformers>=4.35.0",
    "bitsandbytes>=0.42.0",
    "torch>=2.2.0",
    "accelerate>=0.29.0",
    "peft>=0.5.0",
    "xformers>=0.0.29.post1",
    "huggingface-hub"
]

llamacpp = [
    "llama-cpp-python",
]

vllm = [
    "vllm>=0.15.0",
]

service = [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "httpx>=0.26.0",
]

mcp = [
    "mcp[cli]>=1.0.0",
]

search = [
    "ddgs>=7.0.0",  # DuckDuckGo search (free, no API key)
    "tavily-python>=0.5.0",  # Tavily search (production, API key required)
]


[project.scripts]
llm-bawt = "llm_bawt.main:main"
llm = "llm_bawt.main:main"
llm-bawt-service = "llm_bawt.service.server:main"
llm-service = "llm_bawt.service.server:main"
llm-memory = "llm_bawt.memory_debug:main"
llm-memory-tui = "llm_bawt.memory_tui:main"
llm-mcp-server = "llm_bawt.memory_server:run_server"
llm-nextcloud = "llm_bawt.integrations.nextcloud.cli:nextcloud_cli"

[project.entry-points."llm_bawt.memory"]
postgresql = "llm_bawt.memory.postgresql:PostgreSQLMemoryBackend"

[project.entry-points."llm_bawt.agent_backends"]
openclaw = "llm_bawt.agent_backends.openclaw:OpenClawBackend"

[tool.setuptools]
package-dir = {"" = "src"}
include-package-data = true

[tool.setuptools.package-data]
llm_bawt = ["*.yaml"]

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
markers = [
    "service: requires the llm-bawt service running at :8642",
    "llm_call: makes real LLM API calls (slow, uses API credits)",
]
