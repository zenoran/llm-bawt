services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        WITH_CUDA: ${WITH_CUDA:-true}
        CUDA_ARCHS: ${CUDA_ARCHS:-120}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    container_name: llm-bawt-app
    ipc: host  # Required for vLLM multiprocess engine shared memory
    security_opt:
      - apparmor:unconfined  # AppArmor not available on this kernel; nvidia-ctk owns daemon.json
    env_file:
      - ./.env
    ports:
      - "${MCP_PORT:-8001}:8001"
      - "${SERVICE_PORT:-8642}:8642"
    volumes:
      # Mount host config directory to avoid duplication
      - ${HOME}/.config/llm-bawt:/root/.config/llm-bawt
      # Mount logs directory
      - ./.logs:/app/.logs
      # Mount bots.yaml for live updates (nextcloud provisioning, etc.)
      # - ./src/llm_bawt/bots.yaml:/app/src/llm_bawt/bots.yaml:ro  # disabled: Docker Desktop VirtioFS bug
      # Optional: mount local models
      - ${MODELS_PATH:-./models}:/app/models
      # Share host's GGUF model cache to avoid re-downloads
      - ${LLM_BAWT_MODEL_CACHE_DIR:-${HOME}/.cache/llm-bawt/models}:/root/.cache/llm-bawt/models
      # Mount HuggingFace cache for GGUF models
      - ${HF_HOME:-${HOME}/.cache/huggingface}:/root/.cache/huggingface
    restart: unless-stopped
    stdin_open: true
    tty: true
