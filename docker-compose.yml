services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        WITH_CUDA: ${WITH_CUDA:-true}
        CUDA_ARCHS: ${CUDA_ARCHS:-120}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    container_name: llmbothub_app
    env_file:
      - ./.env
    ports:
      - "${MCP_PORT:-8001}:8001"
      - "${SERVICE_PORT:-8642}:8642"
    volumes:
      # Mount host config directory to avoid duplication
      - ${HOME}/.config/llmbothub:/root/.config/llmbothub
      # Mount logs directory
      - ./.logs:/app/.logs
      # Mount bots.yaml for live updates (nextcloud provisioning, etc.)
      - ./src/llmbothub/bots.yaml:/app/src/llmbothub/bots.yaml:ro
      # Optional: mount local models
      - ${MODELS_PATH:-./models}:/app/models
      # Share host's GGUF model cache to avoid re-downloads
      - ${LLMBOTHUB_MODEL_CACHE_DIR:-${HOME}/.cache/llmbothub/models}:/root/.cache/llmbothub/models
      # Mount HuggingFace cache for GGUF models
      - ${HF_HOME:-${HOME}/.cache/huggingface}:/root/.cache/huggingface
    restart: unless-stopped
    stdin_open: true
    tty: true
